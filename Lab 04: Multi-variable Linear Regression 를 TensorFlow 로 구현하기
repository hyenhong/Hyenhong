import tensorflow as tf #텐서플로우 실행
import numpy as np      #numpy 실행 

tf.enable_eager_execution() #바로 실행을 위해 eager_execution로 활성화 
tf.__version__



random 초기화: tf.random_normal()

data and label
x1 = [ 73.,  93.,  89.,  96.,  73.] #실행 코드의 데이터들 
x2 = [ 80.,  88.,  91.,  98.,  66.] 
x3 = [ 75.,  93.,  90., 100.,  70.] 
Y  = [152., 185., 180., 196., 142.]  

# random weights
w1 = tf.Variable(tf.random_normal([1])) # 데이터들의 초기값이 모두 1로 주어짐, 변수가 3개로 주어짐
w2 = tf.Variable(tf.random_normal([1]))
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1])) # b는 한개

learning_rate = 0.000001 # 0.000001 이라는 작은값을 learning_rate로 설정

for i in range(1000+1): # 1000+1번의 수행 
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape: # GradientTape 사용
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b # 위에 주어진 것들로 가설값을 설정
        cost = tf.reduce_mean(tf.square(hypothesis - Y)) #cost function은 가설값에서 실제값을 뺀 즉 오차를 제곱의 평균값으로 설정
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b]) #cost 함수에 대한 w1, w2, w3, b 이 변수들에 대한 기울값
    
    # update w1,w2,w3 and b # 4개의 기울기 값을 각각의 변수에 update
    w1.assign_sub(learning_rate * w1_grad) # 업데이트 할때 w1 그레디언트 값에 learning_rate 값을 곱한다음 그 값을 빼서 할당
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
      print("{:5} | {:12.4f}".format(i, cost.numpy())) # 중간에 50번 마다 cost 값을 출력
      
     
     
     Matrix 사용
      
      data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]  # 5행 3열짜리의 데이터, x= input
y = data[:, [-1]] # 5행 1열짜리의 데이터, y= 출력, 정답

W = tf.Variable(tf.random_normal([3, 1])) #출력은 하나, 입력된 변수는 3개 
b = tf.Variable(tf.random_normal([1]))    #b값은 하나 

learning_rate = 0.000001 # 0.000001 이라는 작은 값으로 learning_rate 설정

# hypothesis, prediction function
def predict(X):
    return tf.matmul(X, W) + b #predict 함수는 X,W로 정의 , 나중에 b의 값은 생략가능

print("epoch | cost")

n_epochs = 2000 # 2000번의 순회를 함
for i in range(n_epochs+1): #2001번을 순회하도록 epochs를 실행
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad) # W값을 지속적으로 updates
    b.assign_sub(learning_rate * b_grad) # b값을 지속적으로 updates
    
    if i % 100 == 0: # 100번에 한번씩 값 확인
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
